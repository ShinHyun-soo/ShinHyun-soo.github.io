---
제목 : 1bit llms
날짜 : 2024 년 3 월 5 일 19시 10:00 -0400 
카테고리 : DL
---

본 논문은 모든 매개변수를 -1, 0, 1 로 바꾸어 LLM 을 경량화 하는 방법에 대해 소개하고 있다.<br>


![image](https://github.com/ShinHyun-soo/GPT/assets/69250097/3a98b9be-736c-4b66-a680-511b6355c1b5)<br>


![image](https://github.com/ShinHyun-soo/GPT/assets/69250097/90cf18d5-505b-4de1-bfc0-8a2b1bad512e)<br>

![image](https://github.com/ShinHyun-soo/GPT/assets/69250097/8fbee833-7039-4dcd-9cb4-bec82aae216c)<br>

그 방법은 가중치 행렬의 각 원소를 해당 행렬의 평균 절댓값으로 나눈 후 -1, 0, 1 중 하나의 정수로 반올림 하는 것이다. <br> (absmean quantization function) <br>

![image](https://github.com/ShinHyun-soo/GPT/assets/69250097/802fd77c-bc95-4001-8e96-e2556d4d7988)<br>

BitNet b1.58 3.9B 모델은 2.4배 빠르고 GPU 메모리를 3.32배 덜 사용하지만 LLaMA 보다 성능은 훨씬 더 좋다고 한다.<br>

![image](https://github.com/ShinHyun-soo/GPT/assets/69250097/875100cd-8553-4fd4-9573-5b560d68d184) <br>

모델 사이즈를 70B 로 키우고 실험한 결과, 4.1 배 더 빨랐다고 주장. <br>

![image](https://github.com/ShinHyun-soo/GPT/assets/69250097/6b222414-de95-4a5a-9dc5-abff6a7515a4) <br>
BitNet b1.58 대부분은 INT8 덧셈 계산인 반면, LLaMA 는 FP16 덧셈이나 FP16 곱셈으로 이루어짐. <br>
그리하여 7nm 칩에서 행렬 곱을 위한 산술 연산 에너지 소비를 71.4 배 절약함. <br>

[The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/pdf/2402.17764.pdf), Shuming ma et al., arXiv 2024 
